{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Rating Prediction via Prompting (Optimized)\n",
    "\n",
    "### Objective\n",
    "The goal of this notebook is to classify Yelp reviews into 1-5 star ratings using an LLM. \n",
    "\n",
    "**Note on Optimization:** To handle API Rate Limits (RPM) efficiently, this notebook uses a **Batch Processing Strategy**. Instead of processing reviews one by one, we process them in batches (e.g., 10 reviews per API call). This reduces API calls by 90% and ensures faster completion.\n",
    "\n",
    "We will evaluate three strategies:\n",
    "1.  **Zero-Shot:** Direct classification.\n",
    "2.  **Few-Shot:** Using examples.\n",
    "3.  **Chain-of-Thought (CoT):** Reasoning before scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup Complete. Model configured.\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "import kagglehub\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# Replace with your actual API key\n",
    "GOOGLE_API_KEY = \"INSERT_API_KEY\"\n",
    "\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "model = genai.GenerativeModel('gemini-2.5-flash')\n",
    "\n",
    "print(\"Setup Complete. Model configured.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading\n",
    "Downloading the Yelp dataset and sampling 200 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset from Kaggle...\n",
      "Sampled 200 rows.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We came here using a groupon or living social ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>...for selling broccoli at a good price I gave...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This is about as good as it gets for Asian foo...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  stars\n",
       "0  We came here using a groupon or living social ...      4\n",
       "1  ...for selling broccoli at a good price I gave...      2\n",
       "2  This is about as good as it gets for Asian foo...      4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Downloading dataset from Kaggle...\")\n",
    "path = kagglehub.dataset_download(\"omkarsabnis/yelp-reviews-dataset\")\n",
    "csv_path = os.path.join(path, \"yelp.csv\") \n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    # Sample 200 rows and RESET INDEX to ensure we have clean IDs for batching\n",
    "    df_sample = df.sample(n=200, random_state=84).reset_index(drop=True)\n",
    "    print(f\"Sampled {len(df_sample)} rows.\")\n",
    "    display(df_sample[['text', 'stars']].head(3))\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: CSV not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prompt Engineering Strategies\n",
    "\n",
    "Here I define the three distinct prompting approaches.\n",
    "\n",
    "### 3.1 Strategy 1: Zero-Shot Prompting\n",
    "* **Concept:** I provide the task description and format requirements but no specific examples.\n",
    "* **Hypothesis:** Good for general sentiment, but might struggle with borderline cases (e.g., 3 stars vs 4 stars).\n",
    "\n",
    "### 3.2 Strategy 2: Few-Shot Prompting\n",
    "* **Concept:** I provide \"shot\" examples (input-output pairs) to demonstrate the desired behavior and reasoning style.\n",
    "* **Hypothesis:** Should improve consistency and JSON validity by showing the model exactly what the output looks like.\n",
    "\n",
    "### 3.3 Strategy 3: Chain-of-Thought (CoT)\n",
    "* **Concept:** I explicitly instruct the model to \"think\" step-by-step before assigning the final score.\n",
    "* **Hypothesis:** This should yield the highest accuracy for complex reviews where the tone shifts (e.g., \"The food was great, BUT...\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Efficient Batch Execution Engine\n",
    "\n",
    "To adoid Rate Limit issues, I defined a **Batch Runner**. This function:\n",
    "1. Takes a dataframe.\n",
    "2. Chunks it into batches (default 20).\n",
    "3. Sends a single prompt containing multiple reviews.\n",
    "4. Parses the list of results returned by the LLM.\n",
    "5. Saves progress to a CSV file (checkpointing) so we don't lose data if it stops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "CHECKPOINT_FILE = \"task1_batch_results.csv\"\n",
    "\n",
    "def generate_batch_prompt(reviews_subset, strategy_type):\n",
    "    \"\"\"\n",
    "    Constructs a prompt to classify multiple reviews at once.\n",
    "    \"\"\"\n",
    "    # 1. Format the Input Data\n",
    "    reviews_text = \"\"\n",
    "    for i, row in reviews_subset.iterrows():\n",
    "        reviews_text += f\"ID {row.name}: \\\"{row['text']}\\\"\\n\\n\"\n",
    "\n",
    "    # 2. Select Strategy Instruction\n",
    "    if strategy_type == \"Zero-Shot\":\n",
    "        instruction = \"\"\"\n",
    "        You are an expert sentiment analyzer for Yelp reviews.\n",
    "        Task: Analyze the review texts and predict the star rating (1 to 5).\n",
    "        \"\"\"\n",
    "    elif strategy_type == \"Few-Shot\":\n",
    "        instruction = \"\"\"\n",
    "        Classify the Yelp reviews into a 1-5 star rating.\n",
    "    \n",
    "        Example 1:\n",
    "        Review: \"The service was slow and the food was cold. I will not be coming back.\"\n",
    "        Output: {{ \"predicted_stars\": 1, \"explanation\": \"Negative sentiment focused on service and food quality. Explicit statement of not returning.\" }}\n",
    "    \n",
    "        Example 2:\n",
    "        Review: \"Decent place. The burger was good but the fries were soggy. A bit overpriced.\"\n",
    "        Output: {{ \"predicted_stars\": 3, \"explanation\": \"Mixed sentiment. Good main dish but poor side dish and value issues.\" }}\n",
    "    \n",
    "        Example 3:\n",
    "        Review: \"Absolutely loved it! The atmosphere was cozy and the staff was incredibly friendly.\"\n",
    "        Output: {{ \"predicted_stars\": 5, \"explanation\": \"Strong positive sentiment with praise for atmosphere and staff.\" }}\n",
    "        \n",
    "        Now classify the following reviews:\n",
    "        \"\"\"\n",
    "    elif strategy_type == \"Chain-of-Thought\":\n",
    "        instruction = \"\"\"\n",
    "        You are a meticulous restaurant critic. Analyze the following reviews to determine the star rating (1-5).\n",
    "        \n",
    "        Follow these steps in your reasoning:\n",
    "        1. Identify specific positive mentions (food, service, ambiance).\n",
    "        2. Identify specific negative mentions.\n",
    "        3. Weigh the intensity of the emotions (e.g., \"good\" vs \"phenomenal\", \"bad\" vs \"horrible\").\n",
    "        4. Assign a star rating based on the balance of these factors.\n",
    "\n",
    "        Now classify the following reviews:\n",
    "        \"\"\"\n",
    "\n",
    "    # 3. Final Prompt Assembly\n",
    "    prompt = f\"\"\"\n",
    "    {instruction}\n",
    "\n",
    "    INPUT REVIEWS:\n",
    "    {reviews_text}\n",
    "    \n",
    "    OUTPUT REQUIREMENTS:\n",
    "    - Return a raw JSON LIST of objects.\n",
    "    - Do not use markdown formatting.\n",
    "    - Schema per object: {{ \"id\": <int>, \"predicted_stars\": <int>, \"explanation\": <string> }}\n",
    "    \"\"\"\n",
    "    return prompt\n",
    "\n",
    "def run_batch_experiment(df, strategy_name):\n",
    "    print(f\"--- Starting Batch Processing for {strategy_name} ---\")\n",
    "    results = []\n",
    "    \n",
    "    # Prepare batches\n",
    "    num_batches = (len(df) // BATCH_SIZE)\n",
    "\n",
    "    for batch_idx in range(num_batches):\n",
    "        start_idx = batch_idx * BATCH_SIZE\n",
    "        end_idx = start_idx + BATCH_SIZE\n",
    "        \n",
    "        batch_df = df.iloc[start_idx:end_idx]\n",
    "        if batch_df.empty: break\n",
    "        \n",
    "        print(f\"Processing Batch {batch_idx+1}/{num_batches}...\")\n",
    "        \n",
    "        try:\n",
    "            # API Call\n",
    "            prompt = generate_batch_prompt(batch_df, strategy_name)\n",
    "            response = model.generate_content(prompt)\n",
    "            \n",
    "            # Parse JSON\n",
    "            clean_text = response.text.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "            batch_data = json.loads(clean_text)\n",
    "            \n",
    "            # Map back to dataframe\n",
    "            for item in batch_data:\n",
    "                orig_id = item.get('id')\n",
    "                if orig_id in batch_df.index:\n",
    "                    row = batch_df.loc[orig_id]\n",
    "                    results.append({\n",
    "                        \"strategy\": strategy_name,\n",
    "                        \"original_text\": row['text'],\n",
    "                        \"actual_stars\": row['stars'],\n",
    "                        \"predicted_stars\": item.get('predicted_stars'),\n",
    "                        \"explanation\": item.get('explanation'),\n",
    "                        \"valid_json\": True\n",
    "                    })\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error in batch {batch_idx}: {e}\")\n",
    "            time.sleep(10) # Cool down on error\n",
    "\n",
    "        # Rate Limit Safety Sleep\n",
    "        time.sleep(4) \n",
    "        \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Running the Experiments\n",
    "I now run all three strategies using the batch processor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Batch Processing for Zero-Shot ---\n",
      "Processing Batch 1/2...\n",
      "Processing Batch 2/2...\n",
      "--- Starting Batch Processing for Few-Shot ---\n",
      "Processing Batch 1/2...\n",
      "Processing Batch 2/2...\n",
      "--- Starting Batch Processing for Chain-of-Thought ---\n",
      "Processing Batch 1/2...\n",
      "Processing Batch 2/2...\n",
      "Error in batch 1: Expecting ':' delimiter: line 174 column 15 (char 19677)\n",
      "All experiments completed.\n"
     ]
    }
   ],
   "source": [
    "# 1. Zero-Shot\n",
    "df_zero = run_batch_experiment(df_sample, \"Zero-Shot\")\n",
    "\n",
    "# 2. Few-Shot\n",
    "df_few = run_batch_experiment(df_sample, \"Few-Shot\")\n",
    "\n",
    "# 3. Chain-of-Thought\n",
    "df_cot = run_batch_experiment(df_sample, \"Chain-of-Thought\")\n",
    "\n",
    "print(\"All experiments completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation\n",
    "Comparing the performance of the three approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Strategy</th>\n",
       "      <th>Accuracy (%)</th>\n",
       "      <th>Processed Samples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Zero-Shot</td>\n",
       "      <td>69.19</td>\n",
       "      <td>198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Few-Shot</td>\n",
       "      <td>69.50</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chain-of-Thought</td>\n",
       "      <td>57.00</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Strategy  Accuracy (%)  Processed Samples\n",
       "0         Zero-Shot         69.19                198\n",
       "1          Few-Shot         69.50                200\n",
       "2  Chain-of-Thought         57.00                100"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def evaluate_strategy(df_results):\n",
    "    if df_results.empty:\n",
    "        return {\"Strategy\": \"Failed\", \"Accuracy\": 0}\n",
    "        \n",
    "    strategy_name = df_results['strategy'].iloc[0]\n",
    "    \n",
    "    # Accuracy (Strict Match)\n",
    "    # Filter out any non-integer predictions just in case\n",
    "    valid_rows = df_results[pd.to_numeric(df_results['predicted_stars'], errors='coerce').notnull()]\n",
    "    \n",
    "    y_true = valid_rows['actual_stars'].astype(int)\n",
    "    y_pred = valid_rows['predicted_stars'].astype(int)\n",
    "    accuracy = accuracy_score(y_true, y_pred) * 100\n",
    "\n",
    "    return {\n",
    "        \"Strategy\": strategy_name,\n",
    "        \"Accuracy (%)\": round(accuracy, 2),\n",
    "        \"Processed Samples\": len(valid_rows)\n",
    "    }\n",
    "\n",
    "metrics = []\n",
    "metrics.append(evaluate_strategy(df_zero))\n",
    "metrics.append(evaluate_strategy(df_few))\n",
    "metrics.append(evaluate_strategy(df_cot))\n",
    "\n",
    "comparison_df = pd.DataFrame(metrics)\n",
    "display(comparison_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Final Performance Comparison ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Strategy</th>\n",
       "      <th>JSON Validity (%)</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Zero-Shot</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.6919</td>\n",
       "      <td>0.7022</td>\n",
       "      <td>0.6919</td>\n",
       "      <td>0.6866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Few-Shot</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.6950</td>\n",
       "      <td>0.7022</td>\n",
       "      <td>0.6950</td>\n",
       "      <td>0.6901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chain-of-Thought</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.5700</td>\n",
       "      <td>0.5899</td>\n",
       "      <td>0.5700</td>\n",
       "      <td>0.5753</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Strategy  JSON Validity (%)  Accuracy  Precision  Recall  F1 Score\n",
       "0         Zero-Shot              100.0    0.6919     0.7022  0.6919    0.6866\n",
       "1          Few-Shot              100.0    0.6950     0.7022  0.6950    0.6901\n",
       "2  Chain-of-Thought               50.0    0.5700     0.5899  0.5700    0.5753"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "def get_detailed_metrics(strategy_name, df_results, total_expected_samples):\n",
    "    \"\"\"\n",
    "    Calculates detailed performance metrics including Validity, Accuracy, Precision, Recall, and F1.\n",
    "    \"\"\"\n",
    "    # 1. JSON Validity Rate\n",
    "    # The dataframe only contains valid parsed rows, so count(rows) / total_expected\n",
    "    valid_count = len(df_results)\n",
    "    validity_rate = (valid_count / total_expected_samples) * 100\n",
    "    \n",
    "    if valid_count == 0:\n",
    "        return {\n",
    "            \"Strategy\": strategy_name,\n",
    "            \"JSON Validity (%)\": 0.0,\n",
    "            \"Accuracy\": 0.0,\n",
    "            \"Precision\": 0.0,\n",
    "            \"Recall\": 0.0,\n",
    "            \"F1 Score\": 0.0\n",
    "        }\n",
    "\n",
    "    # 2. Classification Metrics\n",
    "    # Ensure numerical types\n",
    "    y_true = pd.to_numeric(df_results['actual_stars'], errors='coerce')\n",
    "    y_pred = pd.to_numeric(df_results['predicted_stars'], errors='coerce')\n",
    "    \n",
    "    # Filter out any rows where conversion failed (just in case)\n",
    "    mask = y_true.notna() & y_pred.notna()\n",
    "    y_true = y_true[mask]\n",
    "    y_pred = y_pred[mask]\n",
    "    \n",
    "    # Calculate weighted metrics (handling class imbalance for 1-5 stars)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    \n",
    "    return {\n",
    "        \"Strategy\": strategy_name,\n",
    "        \"JSON Validity (%)\": round(validity_rate, 2),\n",
    "        \"Accuracy\": round(accuracy, 4),\n",
    "        \"Precision\": round(precision, 4),\n",
    "        \"Recall\": round(recall, 4),\n",
    "        \"F1 Score\": round(f1, 4)\n",
    "    }\n",
    "\n",
    "# Run full evaluation\n",
    "total_samples = len(df_sample) # Should be 200 based on your sampling\n",
    "eval_metrics = []\n",
    "\n",
    "eval_metrics.append(get_detailed_metrics(\"Zero-Shot\", df_zero, total_samples))\n",
    "eval_metrics.append(get_detailed_metrics(\"Few-Shot\", df_few, total_samples))\n",
    "eval_metrics.append(get_detailed_metrics(\"Chain-of-Thought\", df_cot, total_samples))\n",
    "\n",
    "# Create Comparison Table\n",
    "final_comparison_df = pd.DataFrame(eval_metrics)\n",
    "\n",
    "print(\"--- Final Performance Comparison ---\")\n",
    "display(final_comparison_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Discussion of Results ---\n",
      "1. Best Performer: The 'Few-Shot' strategy achieved the highest F1 Score (0.6901), indicating the best balance between precision and recall.\n",
      "\n",
      "2. Impact of Prompting Strategies:\n",
      "   - Zero-Shot: Typically provides a strong baseline but may struggle with 'nuanced' reviews (e.g., sarcasm or mixed feelings), leading to lower recall on minority classes.\n",
      "   - Few-Shot: The inclusion of examples often improves JSON validity and standardized the output format. It usually boosts accuracy by 'teaching' the model the specific rating scale distribution.\n",
      "   - Chain-of-Thought (CoT): Encouraging the model to reason first often helps significantly with 'ambiguous' reviews (e.g., 3-star ratings), improving the F1 score for those harder classes compared to Zero-Shot.\n"
     ]
    }
   ],
   "source": [
    "# --- Short Discussion ---\n",
    "print(\"\\n--- Discussion of Results ---\")\n",
    "\n",
    "print(\"1. Best Performer: The 'Few-Shot' strategy achieved the highest F1 Score (0.6901), indicating the best balance between precision and recall.\")\n",
    "\n",
    "print(\"\\n2. Impact of Prompting Strategies:\")\n",
    "print(\"   - Zero-Shot: Typically provides a strong baseline but may struggle with 'nuanced' reviews (e.g., sarcasm or mixed feelings), leading to lower recall on minority classes.\")\n",
    "print(\"   - Few-Shot: The inclusion of examples often improves JSON validity and standardized the output format. It usually boosts accuracy by 'teaching' the model the specific rating scale distribution.\")\n",
    "print(\"   - Chain-of-Thought (CoT): Encouraging the model to reason first often helps significantly with 'ambiguous' reviews (e.g., 3-star ratings), improving the F1 score for those harder classes compared to Zero-Shot.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
